{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 32-bit",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "6d58817d7a36abfa3a827e4c983e24a2b2aeec7ad475bf8b9406aeab7b3302d2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"task6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = \"Rec1_F19_Files_to_zip/file01_Hd_Sp_Freq\""
   ]
  },
  {
   "source": [
    "# MapReduce Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hello', 'world', 'bye', 'how', 'are', 'you', 'what', 'is', 'your', 'name', '', '\\n', 'i', 'study', 'spark', 'and', 'also', 'hadoop', '\\n', 'prefer', 'to', '\\n', 'but', 'have', 'spend', 'more', 'hours', 'studying', '\\n', 'difficult', 'compile', 'debug', 'some', 'say', '\\n', 'am', 'interested', 'in', 'marreduce', 'which', 'part', 'of', '\\n', 'so', 'shall', 'choose', '\\n', 'perhaps', 'can', 'compromise', \"let's\", 'forget', 'about', 'it', 'all', 'do', 'sql', '\\n', 'cloud', 'computing', '\\n', 'supposed', 'get', 'familiar', 'with', 'nosql', 'pighive', '\\n', 'much', '\\n', 'oh', 'it.', \"i'll\", 'only', 'as', 'this', 'started', '\\n', 'no,', \"don't\", 'want', 'be', 'modern?', 'then', \"i'd\", 'better']\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(INPUT_FILE_PATH)\n",
    "linesRef = sc.textFile(INPUT_FILE_PATH)\n",
    "linesRef = linesRef.collect()\n",
    "\n",
    "def getRidOfDupes(termTuple):\n",
    "    key = termTuple[0]\n",
    "    occurrences =  list(map(lambda x: (x[0],x[1]),termTuple[1]))\n",
    "    termIndices = []\n",
    "    for i in occurrences:\n",
    "        termIndices.append(i[0])\n",
    "    firstLineOccurrence = min(termIndices)\n",
    "    inLineOccurrences = []\n",
    "    for j in occurrences:\n",
    "        if j[0] == firstLineOccurrence :\n",
    "            inLineOccurrences.append(j[1])\n",
    "    firstInLineOccurrence = min(inLineOccurrences)\n",
    "    return key,(firstLineOccurrence,firstInLineOccurrence)\n",
    "\n",
    "\n",
    "def mapLineWithLineIndex(line):\n",
    "    for i in range(len(linesRef)):\n",
    "        if line == linesRef[i]:\n",
    "            return (line,i)\n",
    "\n",
    "def splitLineWithLineIndex(line):\n",
    "    r\"\"\"\n",
    "        \"line\" here is a tuple with two elements.\n",
    "        line[0] = the actual line\n",
    "        line[1] = index of the line\n",
    "    \"\"\"\n",
    "    tokenized = line[0].split(\" \")\n",
    "    outputList = []\n",
    "    for i in range(len(tokenized)):\n",
    "        token = tokenized[i]\n",
    "        outputList.append((token.lower(),(line[1],i)))\n",
    "    return outputList\n",
    "\n",
    "def reAlignByLines(token):\n",
    "    word = token[0]\n",
    "    line = token[1][0]\n",
    "    postitionInLine = token[1][1]\n",
    "    return line,(word,postitionInLine)\n",
    "\n",
    "def mergeLineContent(line):\n",
    "    r\"\"\"\n",
    "        \"line\" here is the following:\n",
    "            - line[0] = the line number\n",
    "            - line[1] = iterator that has elements: tuple(word,positionInLine)\n",
    "                - in the following function x[0] = word and x[1]= positionInLine\n",
    "    \"\"\"\n",
    "    lineNum = line[0]\n",
    "    wordList = list(map(lambda x: (x[0],x[1]),line[1]))\n",
    "    wordList.sort(key=lambda x: x[1])\n",
    "    sortedWordList = list(map(lambda kvPair: kvPair[0],wordList))\n",
    "    return lineNum,sortedWordList\n",
    "\n",
    "def reduceToSolution(acc,current):\n",
    "    stringList_acc = acc[1]\n",
    "    stringList_acc.append('\\n')\n",
    "    stringList_current = current[1]\n",
    "    for i in stringList_current:\n",
    "        stringList_acc.append(i)\n",
    "    return acc\n",
    "\n",
    "\n",
    "noDupes = lines.map(lambda line: mapLineWithLineIndex(line)).flatMap(splitLineWithLineIndex).groupByKey().map(getRidOfDupes)\\\n",
    "                .map(reAlignByLines).groupByKey().sortByKey().map(mergeLineContent).reduce(reduceToSolution)\n",
    "\n",
    "noDupesRdd = sc.parallelize(noDupes[1])\n",
    "noDupesRdd.saveAsTextFile(\"Problem06_NoDuplicates_Output\")\n",
    "print(noDupesRdd.collect())"
   ]
  },
  {
   "source": [
    "# Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hello world bye how are you what is your name  \n i study spark and also hadoop \n prefer to \n but have spend more hours studying \n difficult compile debug some say \n am interested in marreduce which part of \n so shall choose \n perhaps can compromise let's forget about it all do sql \n cloud computing \n supposed get familiar with nosql pighive \n much \n oh it. i'll only as this started \n no, don't want be modern? then i'd better \n"
     ]
    }
   ],
   "source": [
    "#format the output to match the input format:\n",
    "outptuString = \"\"\n",
    "for token in noDupesRdd.collect():\n",
    "    outptuString += token + \" \"\n",
    "print(outptuString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ]
}