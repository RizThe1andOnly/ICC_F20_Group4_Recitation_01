{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 32-bit",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "6d58817d7a36abfa3a827e4c983e24a2b2aeec7ad475bf8b9406aeab7b3302d2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"task7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = \"Rec1_F19_Files_to_zip/file01_Hd_Sp_Freq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('i', ['i', 'is', 'it', 'interested', 'in', 'it.', \"i'll\", \"i'd\"]), ('h', ['hadoop', 'how', 'have', 'hours', 'hello']), ('s', ['spark', 'studying', 'study', 'say', 'started', 'spend', 'some', 'so', 'shall', 'sql', 'supposed']), ('b', ['but', 'bye', 'be', 'better']), ('d', ['do', 'debug', 'difficult', \"don't\"]), ('c', ['compile', 'cloud', 'choose', 'can', 'compromise', 'computing']), ('p', ['perhaps', 'prefer', 'part', 'pighive']), ('l', \"let's\"), ('y', ['you', 'your']), ('g', 'get'), (('t', ['to', 'this', 'then']), 'a')]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(INPUT_FILE_PATH)\n",
    "\n",
    "r\"\"\"\n",
    "    Answer Format:\n",
    "        Section_1 - Functions that will be passed into higher order collection\n",
    "        functions like map or reduce. Each function will define its\n",
    "        purpose within itself.\n",
    "\n",
    "        Section_2 - Chain of spark operations to process the data which will take\n",
    "        in the functions defined and apply them to data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Section_1:\n",
    "def mapToFirstLetter(wordTuple):\n",
    "    r\"\"\"\n",
    "        Will extract the first letter of the word and create a tuple that is:\n",
    "        ( firstLetter , (word,occurrence) )\n",
    "\n",
    "    @param:\n",
    "        wordTuple is composed as follows:\n",
    "            - wordTuple[0] = actual word\n",
    "            - wordTuple[1] = number of times word has occurred in doc\n",
    "        \n",
    "    @return:\n",
    "        tuple : ( firstLetter, (word,occurrence) )\n",
    "    \"\"\"\n",
    "\n",
    "    word = wordTuple[0]\n",
    "    occurrence = wordTuple[1]\n",
    "    firstLetter = word[0]\n",
    "    return firstLetter,(word,occurrence)\n",
    "\n",
    "def reduceByFirstLetter(acc,current):\n",
    "    r\"\"\"\n",
    "    @param:\n",
    "        acc : the accumulated val\n",
    "        cuurent: the current tuple being proccessed\n",
    "\n",
    "    @return:\n",
    "        acc the accumalated value which is continuously updated\n",
    "    \n",
    "    @procedure:\n",
    "            Creates a list of all (word,occurrence) tuples \n",
    "        for each letter. The broad process is creating a \n",
    "        list for accumulator (acc) and appending each \n",
    "        current element received.\n",
    "            The accumulator requires an initial value\n",
    "        which is obtained from the very first entry the\n",
    "        reducer. This inital value is a tuple but we\n",
    "        need our accumulator needs to be a list.\n",
    "        Because of this requirement we have an if\n",
    "        statement to test if the \"acc\" parameter is\n",
    "        a tuple and if it is then change it to a list.\n",
    "        This way we can have a list of tuples.\n",
    "            The current parameter can either be a\n",
    "        tuple (word,occurrence) or a list of tuples.\n",
    "        This is due to the parallel nature of processing.\n",
    "        Another reducerByKey task could have already started\n",
    "        processing the elements for the same key. To deal with\n",
    "        this we have a check if \"current\" is a list, in which\n",
    "        case each of that list's elements will be added to \n",
    "        the overall accumulator.\n",
    "    \"\"\"\n",
    "    if isinstance(acc,tuple):\n",
    "        temp = [acc]\n",
    "        acc = temp\n",
    "    \n",
    "    if isinstance(current,list):\n",
    "        for elem in current:\n",
    "            acc.append(elem)\n",
    "        return acc\n",
    "    \n",
    "    acc.append(current)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def finalReducerHelper(letterWordsTuple):\n",
    "    r\"\"\"\n",
    "        Helper method that obtains the 40 most\n",
    "        freuquent words for each letter. Then\n",
    "        creates a tuple that is (firstLetter,\n",
    "        listOf(40 most frequent words)). If\n",
    "        less than 40 words overall then all\n",
    "        of them are included.\n",
    "    \n",
    "    @param:\n",
    "        letterWordTuple :\n",
    "            - [0] : letter\n",
    "            - [1] : list of (word,occurrence) tuples\n",
    "                        - [0] = word\n",
    "                        - [1] = occurrence\n",
    "    \"\"\"\n",
    "    firstLetter = letterWordsTuple[0]\n",
    "    if isinstance(letterWordsTuple[1],tuple):\n",
    "        return firstLetter,letterWordsTuple[1][0]\n",
    "    wordFreqPairs = list(map(lambda wfp: wfp,letterWordsTuple[1]))\n",
    "    wordFreqPairs.sort(key=lambda wfPair: wfPair[1],reverse=True)\n",
    "    topFrequentWords = []\n",
    "    numOfWords = 0\n",
    "    for word in wordFreqPairs:\n",
    "        topFrequentWords.append(word[0])\n",
    "        numOfWords += 1\n",
    "        if numOfWords == 40:\n",
    "            break\n",
    "    return firstLetter,topFrequentWords\n",
    "\n",
    "\n",
    "def finalReducer(acc,current):\n",
    "    r\"\"\"\n",
    "        Get top 40 (or as many as possible if less) most\n",
    "        frequent words for each letter found in the doc.\n",
    "\n",
    "    @param:\n",
    "        acc : most frequent words for each letter\n",
    "        current: tuple (letter, [(word,occurrent),...])\n",
    "            - current[0] = first letter\n",
    "            - current[1] = wordOccurrence Pairs\n",
    "                - wordOccurrence pair;\n",
    "                    - wordOccurrence[0] = word\n",
    "                    - wordOccurrence[1] = occurrence\n",
    "\n",
    "    @return:\n",
    "        acc : top 40 words for each list\n",
    "\n",
    "    @procedure:\n",
    "            For each letter, sort the (word,occurrence) tuple\n",
    "        list by value in decesending order. Then create\n",
    "        outputlist and fill it with the most frequent \n",
    "        words, up to 40 of them.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(acc,tuple): #if this is the very first reduction and initial val is being used\n",
    "        temp = finalReducerHelper(acc)\n",
    "        acc = [temp]\n",
    "    \n",
    "    currentAns = finalReducerHelper(current)\n",
    "    acc.append(currentAns)\n",
    "    return acc\n",
    "    \n",
    "\n",
    "#Section_2:\n",
    "r\"\"\"\n",
    "    \n",
    "The Spark function chain:\n",
    "\n",
    "    The steps here correspond to each line of the mostFrequentWordsPerLetter. Some functions\n",
    "    will be defined before map reduce chain is started (because they are too long for a\n",
    "    lambda function). Those functions will have their notes within the funciton \n",
    "    definition. The steps are as follows :\n",
    "        - flatMapt -> get all the words in 1 list\n",
    "        - map -> map every word to value 1\n",
    "        - reduceByKey -> add up all occurrences for each word\n",
    "        - map -> switch kev value pair to make occurrences keys\n",
    "        - sortByKey -> sorts by occurrences of words so that most frequent words are first\n",
    "        - map -> switch key value pair again to make word the key again\n",
    "        - map -> Uses: mapToFirstLetter, gets first letter of word and uses that as key with word,occurrence as value\n",
    "        - reduceByKey -> Uses: reduceByFirstLetter, collects all of the word,occurrence pairs that belong to a particular first letter\n",
    "        - reduce -> Uses: finalReducer, finds the 40 most frequent words per letter\n",
    "\"\"\"\n",
    "\n",
    "# collection function chain for output: \n",
    "mostFrequentWordsPerLetter = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                                .filter(lambda word: word!='')\\\n",
    "                                .map(lambda word: (word.lower(),1))\\\n",
    "                                .reduceByKey(add)\\\n",
    "                                .map(lambda wordOccurrencePair: (wordOccurrencePair[1],wordOccurrencePair[0]))\\\n",
    "                                .sortByKey(ascending=False)\\\n",
    "                                .map(lambda kvPair: (kvPair[1],kvPair[0]))\\\n",
    "                                .map(mapToFirstLetter)\\\n",
    "                                .reduceByKey(reduceByFirstLetter)\\\n",
    "                                .reduce(finalReducer)\n",
    "                                \n",
    "\n",
    "print(mostFrequentWordsPerLetter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ]
}